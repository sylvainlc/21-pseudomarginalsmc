\subsection{Pseudo marginalisation in Monte Carlo methods}
\label{sec:pseudo:marginalisation}

Pseudo-marginalisation was originally proposed in \cite{beaumont:2003} in the framework of MCMC methods, and in \cite{andrieu:robert:2009} the method was developed further and provided with a solid theoretical basis. In the following we recapitulate briefly the main idea behind this approach. Consider the problem of sampling from some target distribution $\pi$ defined on some measurable space $(\Xset, \Xfd)$ and having a density with respect to some reference measure $\mu$. This density is assumed to be proportional to some \emph{intractable} nonnegative measurable function $\ell$ on $\Xset$, \ie, $\pi(\rmd x) = \lambda(\rmd x) / \lambda \1_\Xset$, where $\lambda(\rmd x) \eqdef \ell(x) \, \mu(\rmd x)$ is finite. While the target density is intractable we assume that there exist some additional state space $(\Zset, \Zfd)$, a Markov kernel $\mathbf{R}$ on $\Xset \times \Zfd$, and some nonnegative measurable function $\Xset \times \Zset \ni (x, z) \mapsto \ell \langle z \rangle (x)$ known up to a constant of proportionality and such that for all $x \in \Xset$, 
\begin{equation} \label{eq:pseudo:marginalisation}
\int \ell \langle z \rangle(x) \, \mathbf{R}(x, \rmd z) = \ell(x). 
\end{equation}
Thus, a pointwise estimate of $\ell(x)$ can be obtained by generating $\zeta$ from $\mathbf{R}(x, \rmd z)$ and computing the statistic $\ell \langle \zeta \rangle(x)$, the \emph{pseudo marginal}. In Monte Carlo methods, the operation of replacing, when necessary, the true marginal $\ell$ by its pseudo marginal is referred to as \emph{pseudo marginalisation}. Interestingly, even though pseudo marginalisation is based on the plug-in principle, it preserves typically the consistency of an algorithm. In order to see the this, let $\bar{\mathsf{X}} \eqdef \mathsf{X} \times \mathsf{Z}$ and $\bar{\mathcal{X}} \eqdef \mathcal{X} \tensprod \mathcal{Z}$; then one may define an extended target distribution $\bar{\pi}(\rmd \bar{x}) \eqdef \bar{\lambda}(\rmd \bar{x}) / \bar{\lambda} \1_{\bar{\mathsf{X}}} = \bar{\lambda}(\rmd \bar{x}) / \lambda \1_{\mathsf{X}}$ on $(\bar{\mathsf{X}}, \bar{\mathcal{X}})$, where 
\begin{equation} \label{eq:extended:target}
\bar{\lambda}(\rmd \bar{x}) \eqdef \ell \langle z \rangle(x) \, \kernel{R}(x, \rmd z) \, \mu(\rmd x)
\end{equation}
(with $\bar{x} = (x, z)$). By \eqref{eq:pseudo:marginalisation}, $\pi$ is the marginal of $\bar{\pi}$ with respect to the $x$ component. This means that we may produce a random sample $(\xi^i)_{i = 1}^N$ in $\Xset$ targeting $\pi$ by generating a sample $(\xi^i, \zeta^i)_{i = 1}^N$ targeting $\bar{\pi}$ and simply discarding the $\Zset$-valued variables $(\zeta^i)_{i = 1}^N$. Let $\rho$ be a Markov transition density on $\Xset$ with respect to the reference measure $\mu$. Then following \cite{andrieu:robert:2009}, a Markov chain $(\xi_m, \zeta_m)_{m \in \nset}$ targeting $\bar{\pi}$ can be produced on the basis of the Metropolis-Hastings algorithm as follows. Given a state $(\xi_m, \zeta_m)$, a candidate $(\xi^\ast, \zeta^\ast)$ for the next state is generated by drawing $\xi^\ast \sim \rho(x) \, \mu(\rmd x)$ and $\zeta^\ast \sim \kernel{R}(\xi^\ast, \rmd z)$ and accepting the same with probability 
$$
\alpha \eqdef 1 \wedge \frac{\ell \langle \zeta^\ast \rangle(\xi^\ast) \rho(\xi^\ast, \xi_m)}{\ell \langle \zeta_m \rangle (\xi_m) \rho(\xi_m, \xi^\ast)},  
$$
which is tractable. Note that $\alpha$ is indeed a pseudo-marginal version of the exact acceptance probability $1 \wedge \ell (\xi^\ast) \rho(\xi^\ast, \xi_m) / \ell (\xi_m) \rho(\xi_m, \xi^\ast)$ obtained if $\ell$ was known. Note that the auxiliary variable $\zeta$ enters $\alpha$ only through the estimates $\ell \langle \zeta^\ast \rangle (\xi^\ast)$ and $\ell \langle \zeta_m \rangle (\xi_m)$, and since the latter has already been computed at the previous iteration there is no need of recomputing this quantity. In the \emph{Monte-Carlo-within-Metropolis algorithm} (see again \cite{andrieu:robert:2009}) $\ell \langle z \rangle(x)$ is a pointwise importance sampling estimate of $\ell(x)$ based on a Monte Carlo sample $\zeta$ generated from $\mathbf{R}(x, \cdot)$. Alternatively, the extended distribution $\bar{\pi}$ can be sampled using rejection sampling or importance sampling, leading to consistent pseudo-marginal formulations of these algorithms as well. 

In the present paper we will generalise the pseudo-marginal approach towards biased estimation by allowing the function 
\begin{equation} \label{eq:biased:pseudo:marginalisation}
    \ell^\precpar(x) \eqdef \int \ell \langle z \rangle(x) \, \mathbf{R}(x, \rmd z)
\end{equation}
on $\Xset$ to be different from $\ell$. Here $\varepsilon \geq 0$ is some accuracy parameter describing the distance between $\ell^\precpar$ and $\ell$. Such biased estimates appear naturally, e.g. when the law $\pi$ is governed by a diffusion process and the density $\ell$ is approximated on the basis of different discretisation schemes; see the next section. In that case, $\precpar$ plays the role of the discretisation step size. In \eqref{eq:biased:pseudo:marginalisation}, also the estimator $\ell \langle z \rangle(x)$ and the kernel $\mathbf{R}(x, \rmd z)$ may depend on $\precpar$, even though this is suppressed in the notation. By introducing the possibly unnormalised measure $\lambda^\varepsilon(\rmd x) \eqdef \ell^\precpar(\rmd x) \, \mu(\rmd x)$ on $\Xfd$ we may define the \emph{skew} target probability measure
$$
    \pi^\varepsilon(\rmd x) \eqdef \frac{\lambda^\precpar(\rmd x)}{\lambda^\precpar \1_\Xset}
$$
on $\Xfd$. In the biased case, generating a sample $(\xi^i, \zeta^i, \omega^i)_{i = 1}^N$ targeting \eqref{eq:extended:target} will, by \eqref{eq:biased:pseudo:marginalisation}, provide a sample $(\xi^i, \omega^i)_{i = 1}^N$ targeting $\pi^\varepsilon$ as a by-product. Thus, it is of utmost importance to obtain control over the bias between $\pi$ and $\pi^\precpar$, which is possible under the assumption that there exists a constant $c \geq 0$ such that for all $h \in \bmf{\Xfd}$ and $\varepsilon$,   
\begin{equation} \label{eq:lipschitz:simple:case}
    \left| \lambda^\varepsilon h - \lambda h \right| \leq c \varepsilon \| h \|_\infty. 
\end{equation}
For instance, in the diffusion process case mentioned above, a condition of type \eqref{eq:lipschitz:simple:case} holds, as we will see in Section~\ref{sec:theoretical:results}, typically true in the case where the density is approximated using the \emph{Durham-Gallant estimator} \cite{durham:gallant:2002}. Using that for all $h \in \bmf{\Xfd}$,  
$$
    \pi^\precpar h - \pi h = \pi^\precpar h \left( 1 - \frac{\lambda^\varepsilon \1_\Xset}{\lambda \1_\Xset} \right) + \frac{\lambda^\precpar h - \lambda h}{\lambda \1_\Xset}, 
$$
we straightforwardly obtain the bound  
$$
    \left| \pi^\precpar h - \pi h \right| \leq \precpar \frac{2 c}{\lambda \1_\Xset} \| h \|_\infty
$$
on the systematic error induced by the skew model. Note that the unbiased case \eqref{eq:pseudo:marginalisation} corresponds to letting $\precpar = 0$ in assumption~\eqref{eq:lipschitz:simple:case}. In the next section we will present a solution to the main problem addressed in Section~\ref{sec:model} exploring a pseudo-marginalised version of the PaRIS discussed in Section~\ref{sec:PaRIS}. 

\subsection{Pseudo-marginal PaRIS}

The algorithm that we will propose relies on the following assumption. 
 
\begin{hypH}
\label{assum:biased:estimate}
Let $(\Zset_n, \Zfd_n)_{n \in \nsetpos}$ be a sequence of general state spaces. For each $n \in \nset$ there exist a Markov kernel $\ukdist{n}$ on $\Xset_n \times \Xset_{n + 1} \times \Zfd_{n + 1}$ and a positive measurable function $\ukest{n}{z}(x_n, x_{n + 1})$ on $\Xset_n \times \Xset_{n + 1} \times \Zset_{n + 1}$ such that for every $x_{n:n + 1} \in \Xset_n \times \Xset_{n + 1}$, drawing $\zeta \sim \ukdist{n}(x_{n:n + 1}, \rmd z)$ and computing $\ukest{n}{\zeta}(x_n, x_{n + 1})$ yields an estimate of $\ud{n}(x_n, x_{n + 1})$.  
\end{hypH}


Under \hypref{assum:biased:estimate}, we denote, for every $n \in \nset$ and $x_{n:n + 1} \in \Xset_n \times \Xset_{n + 1}$, by   
\begin{equation} \label{eq:def:udmod}
\udmod{n}(x_n, x_{n + 1}) \eqdef \int \ukdist{n}(x_{n:n + 1}, \rmd z) \, \ukest{n}{z}(x_n, x_{n + 1})
\end{equation}
the expectation of the statistic $\ukest{n}{\zeta}(x_n, x_{n + 1})$. Here $\precpar$ is an accuracy parameter belonging to some parameter space $\precparsp \subset \rset$ and controlling the bias of the estimated model with respect to the true model; this will be discussed in depth in Section~\ref{sec:lipschitz:continuity}. For each $n \in \nset$ we define the unnormalised kernel 
\begin{equation} \label{eq:def:ukmod}
    \ukmod{n}(x_n, \rmd x_{n + 1}) = \udmod{n}(x_n, x_{n + 1}) \, \mu_{n + 1}(\rmd x_{n + 1}) 
\end{equation}
on $\Xset_n \times \Xfd_{n + 1}$. 
 
%%%%%%
%% pmFS
%%%%%%

\subsubsection{Pseudo-marginal forward sampling}
\label{eq:sec:pm:forward:sampling}

In the case where each $\ud{n}$ is intractable, so is the mixture distribution $\partmixt_n$ defined in \eqref{eq:def:partmixt}. Under \hypref{assum:biased:estimate}, we may, as in Section~\ref{sec:pseudo:marginalisation}, aim at consistent pseudo-marginalisation of the forward-sampling operation by applying self-normalised importance sampling to the extended mixture  
$$
\bar{\pi}_n(i, \rmd x, \rmd z)
\propto \ewght{n}{i} \ukest{n}{z}(\epart{n}{i}, x) \, \mu_{n + 1}(\rmd x) \, \ukdist{n}(\epart{n}{i}, x, \rmd z) 
$$
on $\bar{\Xfd}_{n + 1} \eqdef \powerset{\intvect{1}{\N}} \tensprod \Xfd_{n + 1} \tensprod \Zfd_{n + 1}$ using the instrumental distribution  
$$
\bar{\rho}_n(i, \rmd x, \rmd z) \propto \ewght{n}{i} \adjfuncforward{n}(\epart{n}{i}) \, \prop{n}(\epart{n}{i}, \rmd x) \, \ukdist{n}(\epart{n}{i}, x, \rmd z)
$$
on the same space. Note that the marginal of $\bar{\pi}_n$ with respect to $(i, x)$ is the distribution proportional to $\ewght{n}{i} \ukmod{n}(\epart{n}{i}, \rmd x)$, whose distance to the target $\pi_n$ of interest is controlled by the precision parameter $\precpar$. Here the adjustment multiplier $\adjfuncforward{n}$ and the proposal kernel $\prop{n}$ of the instrumental distribution are as in Section~\ref{sec:SMC}. Each draw from $\bar{\rho}_n$ is assigned an importance weight given by the (tractable) Radon--Nikodym derivative of $\bar{\pi}_n$ with respect to $\bar{\rho}_n$. It is easily seen that this sampling operation, which is detailed in Algorithm~\ref{alg:pm:SMC}, corresponds to replacing the intractable transition density $\ud{n}$ on Line~4 in Algorithm~\ref{alg:ideal:SMC} by an estimate provided by \hypref{assum:biased:estimate}; we will hence refer to Algorithm~\ref{alg:pm:SMC} as \emph{pseudo-marginal forward sampling} and express it concisely as 
$$
    (\epart{n + 1}{i}, \ewght{n + 1}{i})_{i = 1}^\N \sim \mathsf{pmFS}((\epart{n}{i}, \ewght{n}{i})_{i = 1}^\N). 
$$

\begin{algorithm}[h] 
    \KwData{$(\epart{n}{i}, \ewght{n}{i})_{i = 1}^\N$}
    \KwResult{$(\epart{n + 1}{i}, \ewght{n + 1}{i})_{i = 1}^\N$}
    \For {$i = 1 \to \N$}{
        draw $\ind{n + 1}{i} \sim \cat(\{ \adjfuncforward{n}(\epart{n}{\ell}) \ewght{n}{\ell} \}_{\ell = 1}^\N)$\;
        draw $\epart{n + 1}{i} \sim \prop{n}(\epart{n}{{\ind{n + 1}{i}}}, \cdot)$\;
        draw $\zpart{n + 1}{i} \sim \ukdist{n}(\epart{n}{{\ind{n + 1}{i}}}, \epart{n + 1}{i}, \cdot)$\;
        set $\displaystyle 
        \ewght{n + 1}{i} \gets \frac{\ukest{n}{\zpart{n + 1}{i}}(\epart{n}{{\ind{n + 1}{i}}}, \epart{n + 1}{i})}{\adjfuncforward{n}(\epart{n}{{\ind{n + 1}{i}}}) \propdens{n}(\epart{n}{{\ind{n + 1}{i}}}, \epart{n + 1}{i})}$\;
}
\caption{Pseudo-marginal forward sampling, \textsf{pmFS}.} \label{alg:pm:SMC}
\end{algorithm}

Iterating recursively, after initialisation as in Section~\ref{sec:SMC}, pseudo-marginal forward sampling yields a generalisation of the random-weight particle filter proposed in \cite{fearnhead2008particle} in the context of partially observed diffusion processes. 

\subsubsection{Pseudo-marginal backward sampling}
\label{eq:sec:backward:sampling:pseudo:marg}

%Let us turn our focus to backward sampling. 
As intractability of $\ud{n}$ implies intractability of the kernel $\trm{n}$ (defined in \eqref{eq:def:trm}), we aim at further pseudo marginalisation by embedding $\trm{n}$ into the extended probability kernel  
$$
\trmext{n}(i, j, \rmd z) \propto \ewght{n}{j} \ukest{n}{z}(\epart{n}{j}, \epart{n + 1}{i}) \, \ukdist{n}(\epart{n}{j}, \epart{n + 1}{i}, \rmd z)
$$
on $\intvect{1}{\N} \times \powerset{\intvect{1}{\N}} \tensprod \Zfd_{n + 1}$. For every $i$, the marginal of $\trmext{n}(i, \cdot)$ with respect to the $j$ component is, by \eqref{eq:def:udmod}, proportional to $\ewght{n}{j} \udmod{n}(\epart{n}{j}, \epart{n + 1}{i})$, a distribution that we expect to be close to $\trm{n}(i, \cdot)$ for small $\precpar$. The intractable sampling step on Line~3 in Algorithm~\ref{alg:ideal:BS} can therefore be replaced by sampling from $\trmext{n}(i, \cdot)$, after which the auxiliary variables are discarded. The latter sampling operation will be examined in detail in the next section. This approach, which we express concisely as 
$$
    (\tstat[i]{n + 1})_{i = 1}^\N \sim \mathsf{pmBS}((\epart{n}{i}, \tstat[i]{n}, \ewght{n}{i})_{i = 1}^\N, (\epart{n + 1}{i})_{i = 1}^\N), 
$$
is summarised in Algorithm~\ref{alg:pm:backward:sampling}.  

\begin{algorithm}[h] 
    \KwData{$(\epart{n}{i}, \tstat[i]{n}, \ewght{n}{i})_{i = 1}^\N$, $(\epart{n + 1}{i})_{i = 1}^\N$}
    \KwResult{$(\tstat[i]{n + 1})_{i = 1}^\N$}
    \For{$i = 1 \to \N$}{
    \For{$j = 1 \to \K$}{
    draw $( \bi{n + 1}{i}{j}, \zpart{n + 1}{(i, j)}) \sim \trmext{n}(i, \cdot)$\;
    }
    set $\tstat[i]{n + 1} \gets \frac{1}{\K} \sum_{j = 1}^{\K} \left( \tstat[\bi{n + 1}{i}{j}]{n} + \addf{n}(\epart{n}{\bi{n + 1}{i}{j}}, \epart{n + 1}{i}) \right)$\;
}
\caption{Pseudo-marginal backward sampling, \textsf{pmBS}.} \label{alg:pm:backward:sampling}
\end{algorithm}

It remains to discuss how to sample from the extended distribution $\trmext{n}(i, \cdot)$. 
In the following we propose two possible approaches, which can be viewed as pseudo-marginal versions of the techniques discussed in Section~\ref{sec:BS}.  

\subsubsection*{Rejection sampling from $\trmext{n}$} 

Assume that there exists some measurable nonnegative function $c$ on $\Xset_{n + 1}$ such that for all $(x_{n:n + 1}, z) \in \Xset_n \times \Xset_{n + 1} \times \Zset_{n + 1}$,
$
\ukest{n}{z}(x_n, x_{n + 1}) \leq c(x_{n + 1}).  
$
Since this condition allows the Radon--Nikodym derivative of $\trmext{n}(i, \cdot)$ with respect to the probability measure 
\begin{equation} \label{eq:proposal:pm:rejection}
\rho_n^i(j, \rmd z) \propto \ewght{n}{j} \kernel{R}(\epart{n}{j}, \epart{n + 1}{i}, \rmd z)
\end{equation} 
on $\powerset{\intvect{1}{\N}} \tensprod \Zfd_{n + 1}$ to be bounded uniformly as 
$$
\frac{\rmd \trmext{n}(i, \cdot)}{\rmd \rho_n^i}(j, z) \leq \frac{c(\epart{n + 1}{i}) \sumwght{n}}{\sum_{i' = 1}^\N \ewght{n}{i'} \udmod{n}(\epart{n}{i'}, \epart{n + 1}{i})},  
$$ 
we may sample from the target $\trmext{n}(i, \cdot)$ using rejection sampling. Thus, the following procedure is iterated until acceptance: simulate a candidate $(J^\ast, \zeta^\ast)$ from $\rho_n^i$ by drawing $J^\ast \sim \cat(\{ \ewght{n}{j} \}_{j = 1}^\N)$ and $\zeta^\ast \sim \kernel{R}(\epart{n}{J^\ast}, \epart{n + 1}{i}, \rmd z)$; then accept the same with (tractable) probability 
$$
\accprobext \eqdef \frac{\ukest{n}{\zeta^\ast}(\epart{n}{J^\ast}, \epart{n + 1}{i})}{c(\epart{n + 1}{i})}. 
$$ 
Then conditionally to acceptance, the candidate has distribution $\trmext{n}(i, \cdot)$.  
Notably, the probability $\accprobext$ is obtained by simply plugging a transition density estimate provided by \hypref{assum:biased:estimate} into the probability $\accprob$ (see \eqref{eq:std:acc:prob:backward:sampling}) corresponding to the case where $\ud{n}$ is known. Moreover, since the proposal density is independent of $i$, the expected complexity of this sampling schedule is linear in the number of particles (we refer again to \cite{douc:garivier:moulines:olsson:2010}).  

\subsubsection*{MCMC sampling from $\trmext{n}$}

In some cases, bounding the estimator $\ukest{n}{z}(x_n, x_{n + 1})$ uniformly in $z$ and $x_n$ is not possible. Still, we may sample from $\trmext{n}(i, \cdot)$ using the Metropolis-Hastings algorithm with $\rho_n$ (in \eqref{eq:proposal:pm:rejection}) as independent proposal. In this case, $(\bi{n + 1}{i}{j}, \zpart{n + 1}{(i, j)})_{j = 1}^{\K}$ is a Markov chain generated recursively by the following mechanism. Given a state $\bi{n + 1}{i}{j} = J$ and $\zpart{n + 1}{(i, j)} = \zeta$, a candidate $(J^\ast, \zeta^\ast)$ for the next state is drawn from $\rho_n$ (as described above) and accepted with probability 
$$
\accprobext[MH] \eqdef 1 \wedge \frac{\ukest{n}{\zeta^\ast}(\epart{n}{J^\ast}, \epart{n + 1}{i})}{\ukest{n}{\zeta}(\epart{n}{J}, \epart{n + 1}{i})}. 
$$  
In the case of rejection, the next state is assigned the previous state. The resulting Markov chain has $\trmext{n}(i, \cdot)$ as stationary distribution and similar to the case of rejection sampling, the acceptance probability $\accprobext[MH]$ can be viewed as a plug-in estimate of the corresponding probability $\accprob[MH]$ (see \eqref{eq:std:MH:prob:backward:sampling}). 

\subsubsection{Pseudo-marginal PaRIS: full update} 

Combining the pseudo-marginal forward and backward sampling operations yields a pseudo-marginal PaRIS update described in the following algorithm, which is the main contribution of this section.  

\begin{algorithm}[h] 
    \KwData{$(\epart{n}{i}, \tstat[i]{n}, \ewght{n}{i})_{i = 1}^\N$}
    \KwResult{$(\epart{n + 1}{i}, \tstat[i]{n + 1}, \ewght{n + 1}{i})_{i = 1}^\N$}
    run $(\epart{n + 1}{i}, \ewght{n + 1}{i})_{i = 1}^\N \sim \mathsf{pmFS}((\epart{n}{i}, \ewght{n}{i})_{i = 1}^\N)$\;
    run $(\tstat[i]{n + 1})_{i = 1}^\N \sim \mathsf{pmBS}((\epart{n}{i}, \tstat[i]{n}, \ewght{n}{i})_{i = 1}^\N, (\epart{n + 1}{i})_{i = 1}^\N)$\; 
    \medskip
\caption{Full pseudo-marginal PaRIS update.} \label{alg:pm:PaRIS}
\end{algorithm}

Algorithm~\ref{alg:pm:PaRIS} is initialised by drawing $(\epart{0}{i})_{i = 1}^\N \sim \chi^{\varotimes \N}$ and letting $\ewght{0}{i} = \rmd \chi / \rmd \init(\epart{0}{i})$ and $\tstat[i]{n} = 0$.

We now illustrate \hypref{assum:biased:estimate} by a few examples. 

\begin{example}[Durham--Gallant estimator]
\label{eq:durham:gallant}
As an illustration, we return to the state-space model framework discussed in Example~\ref{ex:state-space:models}. Let $\Xset \eqdef \rset^{d_x}$ and $\Yset \eqdef \rset^{d_y}$ be equipped with their respective Borel $\sigma$-fields $\Xfd$ and $\Yfd$, and let $(X_t)_{t > 0}$ be some diffusion process on $\Xset$ driven by the homogeneous stochastic differential equation
\begin{equation} \label{eq:SDE}
\rmd X_t = \mu(X_t) \, \rmd t + \sigma(X_t) \, \rmd W_t, \quad t > 0, 
\end{equation}
where $X_0 = 0$, $(W_t)_{t > 0}$ is $d_x$-dimensional Brownian motion, $b : \Xset \to \Xset$ and $\sigma : \Xset \to \rset^{d_x \times d_x}$ are twice differentiable with bounded first and second order derivatives. In addition, the matrix $\sigma \sigma^\intercal$ is uniformly non-degenerate. Let $(\mathcal{F}_t)_{t > 0}$ be the natural filtration generated by the process $(X_t)_{t > 0}$. The state sequence $(X_t)_{t > 0}$ is latent but partially observed at discrete time points $(t_n)_{n \in \nsetpos}$ which are assumed to be equally spaced for simplicity, \ie, $t_n = t_1 + \delta (n - 1)$ for all $n$ and some $\delta > 0$. Abusing notations, we denote $X_n \eqdef X_{t_n}$ and let $q_\delta$ be the transition density of $(X_n)_{n \in \nsetpos}$. Denote by $\kernel{Q}_\delta$ the transition kernel induced by $q_\delta$. In general, $q_\delta$ is intractable, which makes the problem of computing online, for a given data stream $(y_n)_{n \in \nsetpos}$ in $\Yset$, the sequence of joint-smoothing distributions \eqref{eq:smooth} in models of this sort very challenging. Still, using the Euler scheme, one may, for small $\delta$, approximate $q_\delta$ by 
$$
\bar{q}_\delta(x_n, x_{n + 1}) \eqdef \phi(x_{n + 1}; x_n + \delta \mu(x_n), \delta \sigma^2(x_n)), 
$$ 
where $\phi(\cdot; m, s^2)$ is the density of the Gaussian distribution with mean $m$ and variance $s^2$. Let $\bar{\kernel{Q}}_\delta$ be the transition kernel induced by $\bar{q}_\delta$. Since the approximation $\bar{q}_\delta$ is poor for $\delta$ not small enough, we may instead, as suggested in \cite{durham:gallant:2002}, pick some finer step size $\precpar \in \precparsp_\delta \eqdef \{ \delta / n : n \in \nsetpos \}$ and estimate the density $q_\delta(x_n, x_{n + 1})$ by $q_\delta \langle \zeta \rangle (x_n, x_{n + 1})$, where $\zeta = (\zeta^i)_{i = 1}^L$ are independent draws from some proposal $r(x_n, x_{n + 1}, z) \, \rmd z$ on $\Xset^2 \times \Xfd^{\delta / \precpar - 1}$ and  
$$
q_\delta \langle z \rangle (x_n, x_{n + 1}) \eqdef \frac{1}{L} \sum_{i = 1}^L \frac{\prod_{k = 1}^{\delta / \precpar} \bar{q}_\precpar(z_{k - 1}^i, z_k^i)}{r(x_n, x_{n + 1}, z^i)},  
$$
with $z^i = (z_1^i, \ldots, z_{\delta / \precpar - 1}^i)$ and, by convention, $z_0^i = x_n$ and $z_{\delta / \precpar}^i = x_{n + 1}$. In \cite{durham:gallant:2002}, $r(x_n, x_{n + 1}, z) \, \rmd z$ is the distribution of a discretised (possibly modified) \emph{Brownian bridge}, \ie, Brownian motion started at $x_n$ and conditioned to terminate at $x_{n + 1}$. 

Let $Y_n$ denote the $\Yset$-valued observation at time $t_n$. We will consider two different models for the observation process $(Y_n)_{n \in \nset}$.    

\subsubsection*{Case~1}
First, assume that for all $n \in \nsetpos$, 
$$
Y_n \mid \mathcal{F}_{t_n} \sim \md{n - 1}(X_{n - 1}, X_n, y_n) \, \rmd y_n, 
$$
where $\md{n - 1}$ is some tractable transition density with respect to Lebesgue measure. In this case, \hypref{assum:biased:estimate} holds with the estimator $\ukest{n}{z}(x_n, x_{n + 1}) = q_\delta \langle z \rangle (x_n, x_{n + 1}) \md{n}(x_n, x_{n + 1}, y_{n + 1})$ and the instrumental kernel $\ukdist{n}(x_n, x_{n + 1}, \rmd z) = \prod_{i = 1}^L r(x_n, x_{n + 1}, z^i) \, \rmd z^i$. Finally, we note that 
\begin{equation} \label{eq:ukmod:durham:gallant}
\ukmod{n}(x_n, \rmd x_{n + 1}) = \bar{\kernel{Q}}_\precpar^{\delta / \precpar}(x_n, \rmd x_{n + 1}) \, \md{n}(x_n, x_{n + 1}, y_{n + 1}), 
\end{equation}
which is generally intractable.  

\subsubsection*{Case 2} Alternatively, we may assume that $(Y_n)_{n \in \nsetpos}$ are discrete observations of the solution to the stochastic differential equation 
$$
\rmd Y_t = \tilde{\mu}(X_t, Y_t) \, \rmd t + \tilde{\sigma}(X_t, Y_t) \, \rmd \tilde{W}_t, \quad t > 0, 
$$
where $Y = 0$, $(\tilde{W}_t)_{t > 0}$ is $d_y$-dimensional Brownian motion independent of $(W_t)_{t > 0}$ and $\tilde{\mu} : \Xset \times \Yset \to \Yset$ and $\tilde{\sigma} : \Xset \times \Yset \to \rset^{d_y \times d_y}$ are known functions which are twice differentiable with bounded first and second order derivatives. In addition, the matrix $\tilde{\sigma} \tilde{\sigma}^\intercal$ is uniformly non-degenerate. Denote by $p_\delta$ the transition density of $(X_t, Y_t)_{t > 0}$. In this case, the joint-smoothing distributions can, for a given data stream $(y_n)_{n \in \nsetpos}$, be expressed as path measures \eqref{eq:FK:path} induced by $\ud{n}(x_n, x_{n + 1}) = p_\delta(x_n, y_n, x_{n + 1}, y_{n + 1})$, $n \in \nset$. Since also $p_\delta$ is generally intractable we subject the bivariate process to Euler discretisation, yielding the approximation 
\begin{multline}
\bar{p}_\delta(x_n, y_n, x_{n + 1}, y_{n + 1}) \\
\eqdef \phi \left( x_{n + 1}, y_{n + 1}; (x_n + \delta \mu(x_n), y_n + \delta \tilde{\mu}(x_n, y_n))^\intercal, \delta \, \mbox{diag}(\sigma^2(x_n), \tilde{\sigma}^2(x_n, y_n)) \right) \label{eq:td:bivariate:process}
\end{multline}
of $p_\delta$. Denote by $\bar{\kernel{P}}_\delta$ the Markov kernel induced by $\bar{p}_\delta$. In the case of sparse observations we may improve the approximation $\bar{p}_\delta$ by picking again some finer step size $\precpar \in \mathcal{E}_\delta$ and computing (by swapping, in~\eqref{eq:ukmod:durham:gallant}, $\bar{q}_\delta$ for $\bar{p}_\delta$ and letting $r(x_n, y_n, x_{n + 1}, y_{n + 1}, z) \, \rmd z$ be the distribution of a discretised, $\rset^{d_x + d_y}$-valued Brownian bridge started in $(x_n, y_n)$ and conditioned to terminate in $(x_{n + 1}, y_{n + 1})$) the Durham--Gallant estimator $p_\delta \langle z \rangle$. In this case, $\ukest{n}{z}(x_n, x_{n + 1}) = p_\delta \langle z \rangle (x_n, y_n, x_{n + 1}, y_{n + 1})$, which yields
\begin{equation} \label{eq:ukmod:durham:gallant:case:2}
\ukmod{n}(x_n, \rmd x_{n + 1}) = \bar{p}_\precpar^{\delta / \precpar}(x_n, y_n, x_{n + 1}, y_{n + 1}) \, \rmd x_{n + 1}, 
\end{equation}
where $\bar{p}_\precpar^{\delta / \precpar}$ denotes the transition density of (the $\delta / \precpar$-skeleton) $\bar{\kernel{P}}_\precpar^{\delta / \precpar}$.  
\end{example}

\begin{example}[the exact algorithm] \label{ex:exact:algorithm}
We consider again the partially observed diffusion process model in Example~\ref{eq:durham:gallant}. In the special case where the diffusion process governed by \eqref{eq:SDE} can be transformed into one with a constant diffusion term through the \emph{Lamperti transformation}, it was shown in \cite{beskos:papaspiliopoulos:roberts:fearnhead:2006,fearnhead2008particle} how unbiased estimation of $q_\delta$ can be achieved using generalised Poisson estimators. In our setup, this simply yields $\udmod{n} = \ud{n}$ for all $n$. We refer to the mentioned papers for details. 
\end{example}

\begin{example} 
[approximate Bayesian computation smoothing]
Consider joint smoothing in a fully dominated general state-space HMM for which the state likelihood functions $(\md{n}(\cdot, y_n))_{n \in \nset}$ are intractable (or expensive to evaluate) for any given sequence $(y_n)_{n \in \nset}$ of observations in $\rset^{d_y}$. In the case where it is possible (or faster) to sample observation emissions according to the kernels $(\mk_n)_{n \in \nset}$ one may then 
take an \emph{approximate-Bayesian-computation} (ABC) approach (see {\eg}  \cite{marin:pudlo:robert:ryder:2012}), and replace any value $\md{n}(x_n, y_n)$ by a point estimate $\kappa_\precpar(\zeta_n - y_n)$, where $\zeta_n \sim \mk_n(x_n, \cdot)$ and $\kappa_\varepsilon$ is a $d_y$-dimensional kernel density scaled by some bandwidth $\precpar > 0$. In \cite{martin:jasra:singh:whiteley:delmoral:maccoy:2014}, the authors apply the forward-only smoothing approach of \cite{delmoral:doucet:singh:2010} to this approximate model, yielding a particle-based ABC smoothing algorithm. Also this framework is covered by \hypref{assum:biased:estimate}, by letting $\ukest{n}{z}(x_n, x_{n + 1}) = \hd{n}(x_n, x_{n + 1}) \kappa_\precpar(z - y_{n + 1})$ and $\ukdist{n}(x_n, x_{n + 1}, \rmd z) = \mk_n(x_{n + 1}, \rmd z)$. In this case, 
\begin{equation} \label{eq:ukmod:ABC:smoothing}
\ukmod{n}(x_n, \rmd x_{n + 1}) = \hk_n(x_n, \rmd x_{n + 1}) \int \kappa_\precpar(z - y_{n + 1}) \, \mk_n(x_{n + 1}, \rmd z). 
\end{equation}
\end{example}


