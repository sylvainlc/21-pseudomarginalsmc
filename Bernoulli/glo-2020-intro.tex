Let $(\Xset_n, \Xfd_n)_{n \in \nset}$ be a sequence of general state spaces and let, for all $n \in \nset$, $\uk{n} : \Xset_n \times \Xfd_{n + 1} \to \rsetnn$ be bounded kernels in the sense that $\sup_{x \in \Xset_n} \uk{n}(x, \Xset_{n + 1}) < \infty$. We will assume a dominated model where each kernel $\uk{n}$ has a kernel density $\ud{n}$ with respect to some $\sigma$-finite reference measure $\mu_{n + 1}$ on $\Xfd_{n + 1}$. Finally, let $\chi$ be some bounded measure on $\Xfd_0$. In the following, we denote state-space product sets and $\sigma$-fields by $\Xset^n \eqdef \Xset_0 \times \cdots \times \Xset_n$ and $\Xfd^n \eqdef \Xfd_0 \tensprod \cdots \tensprod \Xfd_n$, respectively, and consider probability measures  
\begin{equation} \label{eq:def:post}
\post{0:n}(\rmd x_{0:n}) \propto \chi(\rmd x_0) \prod_{m = 0}^{n - 1} \uk{m}(x_m, \rmd x_{m + 1}), \quad n \in \nset, 
\end{equation}
on these product spaces.\footnote{We will always use the standard convention $\prod_{\varnothing} = 1$, implying that $\post{0} \propto \chi$.} Given a sequence $(\addf{n})_{n \in \nset}$ of measurable functions $\addf{n} : \Xset_n \times \Xset_{n + 1} \to \rset$, the aim of the present paper is the online approximation of expectations of \emph{additive functionals}   
\begin{equation} \label{eq:add:func}
    h_n : \Xset^n \ni x_{0:n} \mapsto \sum_{m = 0}^{n - 1} \addf{m}(x_m, x_{m + 1})
\end{equation}
under the distribution flow $(\post{0:n})_{n \in \nset}$ using \emph{sequential Monte Carlo} (SMC) methods. 

The generality of the model \eqref{eq:def:post} is striking. In the special case where each $\uk{n}$ can be decomposed as $\uk{n}(x_n, \rmd x_{n + 1}) = \md{n}(x_n) \, \hk_n(x_n, \rmd x_{n + 1})$ for some Markov kernel $\hk_n$ and some nonnegative potential function $\md{n}$, \eqref{eq:def:post} yields the \emph{Feynman-Kac path models} \cite{delmoral:2004}, which are applied in a large variety of scientific and engineering disciplines, including statistics, physics, biology, and signal processing. In a \emph{hidden Markov model} (HMM) (see, \eg,  \cite{Cappe:2005:IHM:1088883}), a Markov chain $(X_n)_{n \in \nset}$ with kernels $(\hk_n)_{n \in \nset}$ and initial distribution $\chi$ is only partially observed through a sequence $(Y_n)_{n \in \nset}$ of observations being conditionally independent given the Markov states. In that case, $g_n$ plays the role of the likelihood of the state $X_n$ given the corresponding observation $Y_n$, and $\post{0:n}$ describes the \emph{joint-smoothing distribution}, \ie, the joint posterior of the hidden states $X_0, \ldots, X_n$ given corresponding observations (see Example~\ref{ex:state-space:models} for details). We will adopt this terminology throughout the present paper and refer to the distributions defined in \eqref{eq:def:post} as `smoothing distributions'; the problem of computing expectations of functionals of type \eqref{eq:add:func} under these distributions will be referred to as `additive smoothing'. General state-space HMMs are prevalent in time-series and sequential-data analysis and are used extensively in, \eg, movement ecology \cite{michelot2016movehmm}, energy-consumption modeling \cite{candanedo2017methodology}, genomics \cite{yau2011bayesian}, target tracking \cite{sarkka2007rao}, enhancement and segmentation of speech and audio signals \cite{rabiner1989tutorial}; see also \cite{sarkka2013bayesian, zucchini2017hidden} and the numerous references therein. Operating on models of this sort, online additive smoothing is instrumental for, \emph{e.g.},    
\begin{enumerate}
    \item[--] \emph{path reconstruction}, \ie, the estimation of hidden states given observations. Especially in \emph{fixed-point smoothing}, where interest is in computing the expectations of $h(X_m)$ conditionally on $Y_0, \ldots, Y_n$ for some given $m$ and test function $h$ as $n$ tends to infinity, a problem that can be cast into our framework by letting, in \eqref{eq:add:func}, $\addf{m}(x_m, x_{m + 1}) = h(x_m)$ and $\addf{\ell} \equiv 0$ for all $\ell \neq m$. 
    \item[--] \emph{parameter inference}, where additive smoothing is a key ingredient in the computation of log-likelihood gradients (\emph{score functions}) via Fisher's identity or the intermediate quantity of the \emph{expectation-maximisation} (EM) \emph{algorithm}; see, \eg, \cite[Chapter~10]{Cappe:2005:IHM:1088883}. On-the-fly computation becomes especially important in online implementations via, \eg, the \emph{online EM} or \emph{recursive maximum likelihood} approaches \cite{cappe:2009,legland:mevel:1997}.  
 \end{enumerate}

As closed-form solutions to this smoothing problem can be obtained only for linear Gaussian models or models with finite state spaces $(\Xset_n)_{n \in \nset}$, loads of papers have been written over the years with the aim of developing SMC-based approximative solutions. Most of these works assume that each density $\ud{n}$ (or, in the HMM case, the transition density of $\hk_n$ and the likelihood $g_n$) is available in a closed form; however, this is not the case for a large number of interesting models, including most state-space HMMs governed by stochastic differential equations. Still, there are a few exceptions in the literature. In \cite{fearnhead2008particle} (see also \cite{fearnhead:papaspiliopoulos:roberts:stuart:2010}), the authors showed that asymptotically consistent online state estimation in partially observed diffusion processes can be achieved by means of a \emph{random-weight particle filter}, in which unavailable importance weights are replaced by unbiased estimates (produced using so-called \emph{generalized Poisson estimators} \cite{beskos:papaspiliopoulos:roberts:fearnhead:2006}). This approach is closely related to \emph{pseudo-marginal methods} \cite{andrieu:robert:2009}, since the unbiasedness allows the true, intractable target to be embedded into an extended distribution having the target as a marginal; as a consequence, the consistency of the algorithm follows straightforwardly from standard SMC convergence results. A similar pseudo-marginal SMC approach was developed in \cite{mcgree:drovandi:white:pettitt:2016} for random effects models with non-analytic likelihood. In \cite{olsson:strojby:2011}, this technology was cast into the framework of \emph{fixed-lag particle smoothing} of additive state functionals, where the well-known particle-path degeneracy of naive particle smoothers is avoided at the price of a lag-induced bias. Recently, \cite{yonekura:beskos:2020} designed a random-weight version of the forward-only particle smoother proposed in \cite{delmoral:doucet:singh:2010}, whose computational complexity is quadratic in the number $\N$ of particles, yielding a strongly consistent---though computationally demanding---algorithm. Moreover, \cite{gloaguen2018online} extended the random-weight particle filtering approach to the \emph{particle-based, rapid incremental smoother} (PaRIS), proposed in \cite{olsson:westerborn:2014b} as a means for additive smoothing in HMMs, yielding an algorithm with just linear complexity. The complexity of the latter algorithm is appealing; however, the schedule was not furnished with any theoretical results concerning the asymptotic properties and long-term stability of the estimator or the effect of the weight randomisation on the accuracy. In addition, the algorithm is restricted to partially observed diffusions and unbiased weight estimation, calling for strong assumptions on the unobserved process. 
        
In the present paper we further develop the approach in \cite{gloaguen2018online} and extend the PaRIS to online additive smoothing in general models in the form \eqref{eq:def:post} and the scenario where the transition densities $(\ud{n})_{n \in \nset}$ are intractable but can be estimated by means of simulation. These estimates may be unbiased or biased. In its original form, the PaRIS avoids particle-path degeneracy by alternating two sampling operations, one that propagates a sample of forward-filtering particles and another that resamples a set of backward-smoothing statistics, and the proposed method replaces the sampling distributions associated with these operations by suitable pseudo-marginals. This leads to an $\mathcal{O}(\N)$ algorithm that can be applied to a wide range of smoothing problems, including additive smoothing in partially observed diffusion processes and additive \emph{approximate Bayesian computation smoothing} \cite{martin:jasra:singh:whiteley:delmoral:maccoy:2014}. As illustrated by our examples, it covers the random-weight algorithms proposed in \cite{fearnhead2008particle} and \cite{gloaguen2018online} as special cases and provides, as another special case, an extension of the original PaRIS proposed in \cite{olsson:westerborn:2014b} to general path models \eqref{eq:def:post} and auxiliary particle filters. In addition, the proposed method is furnished with a rigorous theoretical analysis, the results of which can be summarised as follows. 

\begin{itemize}
\item We establish exponential concentration and asymptotic normality of the estimators produced by the algorithm. These results extend analogous results established in \cite{olsson:westerborn:2014b} for the original PaRIS (operating on fully dominated HMMs using the bootstrap particle filter), and the additional randomness of the pseudo-marginals can be shown to affect the asymptotic variance through an additional positive term. The fact that our smoothing algorithm, as explained above, involves two separate levels of pseudo-marginalisation makes this extension highly non-trivial. 
\item Under strong mixing assumptions we establish the long-term stochastic stability of our algorithm by showing that its asymptotic variance grows at most linearly in $n$. As explained in \cite[Section~1]{olsson:westerborn:2014b}, this is optimal for a path-space Monte Carlo estimator. As a by-product of this analysis, we obtain a time-uniform bound on the asymptotic variance of the random-weight particle filter. 
\item As mentioned above, we do not require the estimators of $(\uk{n})_{n \in \nset}$ to be unbiased. The bias is assumed to be regulated by some precision parameter $\precpar$ (see \hypref{assum:bias:bound}), and under additional strong mixing assumptions we establish an $\mathcal{O}(n \precpar)$ bound on the asymptotic bias of the final estimator. In addition, we obtain, as a by-product, an $\mathcal{O}(\precpar)$ bound for the random-weight particle filter. These results are the first of their kind.   
\end{itemize}

The paper is structured as follows. In Section~\ref{sec:preliminaries} we cast, under the temporary assumption that each $\ud{n}$ is tractable, the PaRIS into the general model \eqref{eq:def:post} and auxiliary particle filters and define carefully the two forward and backward sampling operations constituting the algorithm. Since this extension is of independent interest, we provide the details. In Section~\ref{sec:pseudo:marginal:PaRIS} we show how pseudo-marginal forward and backward sampling allow the temporary tractability assumption to be abandoned. Section~\ref{sec:theoretical:results} presents all theoretical results and although an extensive numerical study of the proposed scheme is beyond the scope of our paper, we present a minor numerical illustration of the $\mathcal{O}(n \precpar)$ bias bound in Section~\ref{sec:numerical:results}. All proofs are found in the supplement. 


 
